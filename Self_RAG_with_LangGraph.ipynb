{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dd7c4c87",
      "metadata": {
        "id": "dd7c4c87"
      },
      "source": [
        "# Self-Reflective Retrieval-Augmented Generation (Self-RAG) with LangGraph"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Goal\n",
        "The goal of this project is to experiment with Self-RAG with LangGraph.\n",
        "\n",
        "This project is based on [\"Self-Reflective RAG with LangGraph\"](https://blog.langchain.dev/agentic-rag-with-langgraph/) blog by LangChain Team."
      ],
      "metadata": {
        "id": "jBHKYqKvb7Qq"
      },
      "id": "jBHKYqKvb7Qq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-RAG Introduction\n",
        "\n",
        "Large Language Models (LLMs), can generate impressive text but often produce **factually incorrect responses** because they rely on **pre-trained knowledge** and don’t dynamically verify facts. To solve this, researchers have developed **Self-Reflective Retrieval-Augmented Generation (Self-RAG)**—a framework that enables LLMs to fetch relevant external information **when needed** and **self-assess** their own outputs for accuracy and reliability."
      ],
      "metadata": {
        "id": "1zJ9KBQmd8iX"
      },
      "id": "1zJ9KBQmd8iX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How Self-RAG Works\n",
        "\n",
        "Self-RAG improves the traditional Retrieval-Augmented Generation (RAG) process by introducing three core components:\n",
        "\n",
        "\n",
        "\n",
        "**1.  Adaptive Retrieval:**\n",
        "\n",
        "* Unlike standard retrieval models, which fetch a fixed number of documents, Self-RAG allows the model to determine when and what information to retrieve based on the complexity of the question.\n",
        "* This helps the model avoid unnecessary information overload and focus only on relevant data.\n",
        "\n",
        "\n",
        "\n",
        "**2.  Response Generation:**\n",
        "\n",
        "* After retrieving information, the LLM integrates it into a coherent and accurate response.\n",
        "\n",
        "**3.  Self-Reflection with Specialized Tokens:**\n",
        "\n",
        "* Self-RAG uses reflection tokens that allow the model to evaluate its own response in real time.\n",
        "* These special tokens are embedded in the model’s reasoning process to check and refine the accuracy of its generated responses."
      ],
      "metadata": {
        "id": "5rzJwAXnegWb"
      },
      "id": "5rzJwAXnegWb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-Reflection Tokens\n",
        "\n",
        "Self-RAG introduces four key reflection tokens, which play a crucial role in the model’s self-reflection and response evaluation:\n",
        "\n",
        "**1. ISREL (Information Relevance Token)**\n",
        "* This token is used to evaluate whether the retrieved information is relevant to the question being asked.\n",
        "* The model asks itself: “Is this document useful for answering the query?”\n",
        "\n",
        "**2. ISSUP (Information Support Token)**\n",
        "* This token helps determine whether the retrieved information supports the generated response.\n",
        "* The model assesses: “Does the information I retrieved actually back up my answer?”\n",
        "* This step ensures that the response is not based on assumptions but on factual sources.\n",
        "\n",
        "**3. ISUSE (Information Usefulness Token)**\n",
        "* This token checks whether the retrieved information was actually used in the response.\n",
        "* The model verifies: “Did I properly incorporate this information into my answer?”\n",
        "* It prevents the model from retrieving documents but then ignoring them in the final response.\n",
        "\n",
        "**4.\tISERR (Information Error Token)**\n",
        "* This token allows the model to detect potential mistakes in its generated response.\n",
        "* The model asks: “Did I make an error in my answer?”\n",
        "* If an error is detected, the model can rephrase, correct, or retrieve more information to improve accuracy."
      ],
      "metadata": {
        "id": "I1b7zKDGfW9u"
      },
      "id": "I1b7zKDGfW9u"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "1. [“Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection\" ](https://arxiv.org/abs/2310.11511)by Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi. 2023\n",
        "\n",
        "2. [\"Self-Reflective RAG with LangGraph\"](https://blog.langchain.dev/agentic-rag-with-langgraph/) blog is available at LangChain.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xyjLw1G3tnd_"
      },
      "id": "xyjLw1G3tnd_"
    },
    {
      "cell_type": "markdown",
      "id": "357c97c9",
      "metadata": {
        "id": "357c97c9"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NH8irGTCpUn",
        "outputId": "21295bf2-6b71-42f9-d6c4-147e063a7b58"
      },
      "id": "6NH8irGTCpUn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd \"your-path-here\""
      ],
      "metadata": {
        "id": "l7EFrvTpTd-l"
      },
      "id": "l7EFrvTpTd-l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install langgraph langchain faiss-cpu langchain-openai langchain_community chromadb\n",
        "!pip install -U langchain langgraph langchain_openai langchain-community"
      ],
      "metadata": {
        "id": "uHV2h7mriRaM"
      },
      "id": "uHV2h7mriRaM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-OpenAI-API-key-here\""
      ],
      "metadata": {
        "id": "YV5rJLSflP0z"
      },
      "id": "YV5rJLSflP0z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Data\n",
        "\n",
        "We will use articles about Quantum Computing in this project:\n",
        "* [Amazon Unveils Its First Quantum Computing Chip](https://www.wsj.com/articles/amazon-unveils-its-first-quantum-computing-chip-750ee5da)\n",
        "* [Google’s New Quantum Computer Chip](https://www.businessinsider.com/google-unveiled-quantum-computer-chip-willow-2024-12)\n",
        "* [The Age of Quantum Software Has Already Started](https://www.wsj.com/articles/the-age-of-quantum-software-has-already-started-854eccfa)"
      ],
      "metadata": {
        "id": "drVQ38XUiz0M"
      },
      "id": "drVQ38XUiz0M"
    },
    {
      "cell_type": "code",
      "source": [
        "urls = [\n",
        "    \"https://www.wsj.com/articles/amazon-unveils-its-first-quantum-computing-chip-750ee5da\",\n",
        "    \"https://www.businessinsider.com/google-unveiled-quantum-computer-chip-willow-2024-12\",\n",
        "    \"https://www.wsj.com/articles/the-age-of-quantum-software-has-already-started-854eccfa\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "n3qm--YOiRcy"
      },
      "id": "n3qm--YOiRcy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=250, chunk_overlap=0\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2Mj0b9ipz92",
        "outputId": "6c50cfd5-6baa-476c-e005-e9f74246810d"
      },
      "id": "-2Mj0b9ipz92",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Embeddings and Vector Store"
      ],
      "metadata": {
        "id": "I7eSLP0Bl7hN"
      },
      "id": "I7eSLP0Bl7hN"
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate embeddings for the documents and store them in a vector store for efficient retrieval\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"rag-chroma\",\n",
        "    embedding=OpenAIEmbeddings(),\n",
        ")"
      ],
      "metadata": {
        "id": "vYuozFaMkKKa"
      },
      "id": "vYuozFaMkKKa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up the Retriever"
      ],
      "metadata": {
        "id": "MGd8HtyTmBRa"
      },
      "id": "MGd8HtyTmBRa"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up a retriever from the vector store to fetch relevant documents based on queries\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "DaaXcQcNiRfZ"
      },
      "id": "DaaXcQcNiRfZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLMs"
      ],
      "metadata": {
        "id": "XMAEkYsSqlfZ"
      },
      "id": "XMAEkYsSqlfZ"
    },
    {
      "cell_type": "code",
      "source": [
        "### Retrieval Grader\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "#from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "# Data model\n",
        "class GradeDocuments(BaseModel):\n",
        "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
        "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
        "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
        "grade_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "retrieval_grader = grade_prompt | structured_llm_grader\n",
        "question = \"Please explain shortly Amazon's first quantum computing chip features\"\n",
        "docs = retriever.get_relevant_documents(question)\n",
        "doc_txt = docs[1].page_content\n",
        "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXlZ1i_WiRkU",
        "outputId": "6d3cd449-1a0b-44ba-c6f7-440939663d1d"
      },
      "id": "RXlZ1i_WiRkU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py:1390: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "binary_score='yes'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Generate\n",
        "\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import textwrap\n",
        "\n",
        "# Prompt\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# Chain\n",
        "rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run\n",
        "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
        "\n",
        "# Print question and response\n",
        "print(\"\\nQuestion: %s\" % question)\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Print formatted final answer\n",
        "wrapped_answer = textwrap.fill(generation, width=80)  # Wrap text for readability\n",
        "print(\"\\nAnswer:\\n\" + wrapped_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JchXu53iRm1",
        "outputId": "64449d26-2413-4c5b-8714-a9a788e750c5"
      },
      "id": "5JchXu53iRm1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: Please explain shortly Amazon's first quantum computing chip features\n",
            "--------------------------------------------------\n",
            "\n",
            "Answer:\n",
            "Amazon's first quantum computing chip, Ocelot, can reduce quantum computing\n",
            "errors by up to 90%, leading to more reliable quantum computers. The chip is a\n",
            "significant step in the development of useful and cost-effective quantum\n",
            "computing technology. Amazon's cloud-computing business claims that Ocelot can\n",
            "lower the costs associated with reducing quantum computing errors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Please describe the main features of Google’s New Quantum Computer Chip\"\n",
        "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
        "\n",
        "# Print question and response\n",
        "print(\"\\nQuestion: %s\" % question)\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Print formatted final answer\n",
        "wrapped_answer = textwrap.fill(generation, width=80)  # Wrap text for readability\n",
        "print(\"\\nAnswer:\\n\" + wrapped_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqzQgKrkiRsA",
        "outputId": "c5b163c4-babe-4cdb-cbf2-5da4507d6809"
      },
      "id": "fqzQgKrkiRsA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: Please describe the main features of Google’s New Quantum Computer Chip\n",
            "--------------------------------------------------\n",
            "\n",
            "Answer:\n",
            "Google's new quantum computer chip, called Willow, reduces errors and\n",
            "outperforms standard benchmarks. The chip can perform a standard benchmark\n",
            "computation in under five minutes, a task that would take current supercomputers\n",
            "10 septillion years. This development represents a key milestone in the race to\n",
            "build accurate quantum computers with practical applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Please compare Google’s new quantum computer chip with Amazon's first quantum computing chip\"\n",
        "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
        "\n",
        "# Print question and response\n",
        "print(\"\\nQuestion: %s\" % question)\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Print formatted final answer\n",
        "wrapped_answer = textwrap.fill(generation, width=80)  # Wrap text for readability\n",
        "print(\"\\nAnswer:\\n\" + wrapped_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GycJk2Z4iRuG",
        "outputId": "5f2f9216-1bd3-459d-f411-703bcfc9c99c"
      },
      "id": "GycJk2Z4iRuG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: Please compare Google’s new quantum computer chip with Amazon's first quantum computing chip\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Answer:\n",
            "Google's new quantum computer chip, Willow, vastly outperforms standard\n",
            "benchmarks and reduces errors significantly. Amazon's first quantum computing\n",
            "chip, Ocelot, is claimed to lower the costs of reducing quantum computing errors\n",
            "by up to 90%. Both chips represent significant advancements in quantum computing\n",
            "technology.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hallucination Grader"
      ],
      "metadata": {
        "id": "Iz3nQUcxKiBJ"
      },
      "id": "Iz3nQUcxKiBJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Data model\n",
        "class GradeHallucinations(BaseModel):\n",
        "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
        "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
        "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
        "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8-99H-uIVPK",
        "outputId": "a3b2e750-f469-4a0d-f22f-417bf0749cd6"
      },
      "id": "o8-99H-uIVPK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py:1390: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradeHallucinations(binary_score='yes')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer Grader"
      ],
      "metadata": {
        "id": "I7XngJIoLU9b"
      },
      "id": "I7XngJIoLU9b"
    },
    {
      "cell_type": "code",
      "source": [
        "# Data model\n",
        "class GradeAnswer(BaseModel):\n",
        "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "\n",
        "# LLM with function call\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n\n",
        "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
        "answer_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "answer_grader = answer_prompt | structured_llm_grader\n",
        "answer_grader.invoke({\"question\": question, \"generation\": generation})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgSjcs8NLSQp",
        "outputId": "db0dcff9-57ef-40fb-b7b3-8178f8400f64"
      },
      "id": "TgSjcs8NLSQp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py:1390: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradeAnswer(binary_score='yes')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question Re-writer"
      ],
      "metadata": {
        "id": "gnfhArsgLj-a"
      },
      "id": "gnfhArsgLj-a"
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are question re-writer that converts an input question to a better version that is optimized \\n\n",
        "     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
        "re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
        "question_rewriter.invoke({\"question\": question})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "g867UqhALt6Z",
        "outputId": "a36bbb77-7cb2-4424-8a53-c556d39db827"
      },
      "id": "g867UqhALt6Z",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Compare the features and performance of Google's latest quantum computer chip with Amazon's initial quantum computing chip.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph"
      ],
      "metadata": {
        "id": "E-d99G_LMiGy"
      },
      "id": "E-d99G_LMiGy"
    },
    {
      "cell_type": "code",
      "source": [
        "# Graph State\n",
        "from typing import List\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        question: question\n",
        "        generation: LLM generation\n",
        "        documents: list of documents\n",
        "    \"\"\"\n",
        "\n",
        "    question: str\n",
        "    generation: str\n",
        "    documents: List[str]"
      ],
      "metadata": {
        "id": "JY5Y5ihdMkcL"
      },
      "id": "JY5Y5ihdMkcL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Nodes\n",
        "\n",
        "def retrieve(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---RETRIEVE---\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Retrieval\n",
        "    documents = retriever.get_relevant_documents(question)\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "\n",
        "def generate(state):\n",
        "    \"\"\"\n",
        "    Generate answer\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation, that contains LLM generation\n",
        "    \"\"\"\n",
        "    print(\"---GENERATE---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # RAG generation\n",
        "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "\n",
        "\n",
        "def grade_documents(state):\n",
        "    \"\"\"\n",
        "    Determines whether the retrieved documents are relevant to the question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with only filtered relevant documents\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Score each doc\n",
        "    filtered_docs = []\n",
        "    for d in documents:\n",
        "        score = retrieval_grader.invoke(\n",
        "            {\"question\": question, \"document\": d.page_content}\n",
        "        )\n",
        "        grade = score.binary_score\n",
        "        if grade == \"yes\":\n",
        "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "            filtered_docs.append(d)\n",
        "        else:\n",
        "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "            continue\n",
        "    return {\"documents\": filtered_docs, \"question\": question}\n",
        "\n",
        "\n",
        "def transform_query(state):\n",
        "    \"\"\"\n",
        "    Transform the query to produce a better question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates question key with a re-phrased question\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---TRANSFORM QUERY---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Re-write question\n",
        "    better_question = question_rewriter.invoke({\"question\": question})\n",
        "    return {\"documents\": documents, \"question\": better_question}\n",
        "\n",
        "\n",
        "### Edges\n",
        "\n",
        "\n",
        "def decide_to_generate(state):\n",
        "    \"\"\"\n",
        "    Determines whether to generate an answer, or re-generate a question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Binary decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
        "    state[\"question\"]\n",
        "    filtered_documents = state[\"documents\"]\n",
        "\n",
        "    if not filtered_documents:\n",
        "        # All documents have been filtered check_relevance\n",
        "        # We will re-generate a new query\n",
        "        print(\n",
        "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
        "        )\n",
        "        return \"transform_query\"\n",
        "    else:\n",
        "        # We have relevant documents, so generate answer\n",
        "        print(\"---DECISION: GENERATE---\")\n",
        "        return \"generate\"\n",
        "\n",
        "\n",
        "def grade_generation_v_documents_and_question(state):\n",
        "    \"\"\"\n",
        "    Determines whether the generation is grounded in the document and answers question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK HALLUCINATIONS---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    generation = state[\"generation\"]\n",
        "\n",
        "    score = hallucination_grader.invoke(\n",
        "        {\"documents\": documents, \"generation\": generation}\n",
        "    )\n",
        "    grade = score.binary_score\n",
        "\n",
        "    # Check hallucination\n",
        "    if grade == \"yes\":\n",
        "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
        "        # Check question-answering\n",
        "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
        "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
        "        grade = score.binary_score\n",
        "        if grade == \"yes\":\n",
        "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
        "            return \"useful\"\n",
        "        else:\n",
        "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
        "            return \"not useful\"\n",
        "    else:\n",
        "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
        "        return \"not supported\""
      ],
      "metadata": {
        "id": "V5ipax45MxdU"
      },
      "id": "V5ipax45MxdU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Graph"
      ],
      "metadata": {
        "id": "nrHxUOpVNI-B"
      },
      "id": "nrHxUOpVNI-B"
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import END, StateGraph, START\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
        "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
        "workflow.add_node(\"generate\", generate)  # generatae\n",
        "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
        "\n",
        "# Build graph\n",
        "workflow.add_edge(START, \"retrieve\")\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate,\n",
        "    {\n",
        "        \"transform_query\": \"transform_query\",\n",
        "        \"generate\": \"generate\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"generate\",\n",
        "    grade_generation_v_documents_and_question,\n",
        "    {\n",
        "        \"not supported\": \"generate\",\n",
        "        \"useful\": END,\n",
        "        \"not useful\": \"transform_query\",\n",
        "    },\n",
        ")\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "e-oNyE6FNIK6"
      },
      "id": "e-oNyE6FNIK6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "inputs = {\"question\": \"What are the main features of Amazon's first quantum computing chip?\"}\n",
        "\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Print the node name\n",
        "        print(f\"\\n Node '{key}':\")\n",
        "\n",
        "        # Optional: Print full state at each node (formatted for readability)\n",
        "        if \"keys\" in value:\n",
        "            formatted_text = textwrap.fill(str(value[\"keys\"]), width=80, subsequent_indent=\"    \")\n",
        "            print(f\"   State: {formatted_text}\")\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 80 + \"\\n\")  # Separator for clarity\n",
        "\n",
        "# Final generation output with text wrapping\n",
        "if \"generation\" in value:\n",
        "    print(\"\\n Final Generated Response:\\n\")\n",
        "    print(textwrap.fill(value[\"generation\"], width=80, subsequent_indent=\"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Apd-t6y-Nx2o",
        "outputId": "32e93fd6-fe53-456b-9c46-bee5f91956d6"
      },
      "id": "Apd-t6y-Nx2o",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---RETRIEVE---\n",
            "\n",
            " Node 'retrieve':\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: GENERATE---\n",
            "\n",
            " Node 'grade_documents':\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "---GENERATE---\n",
            "---CHECK HALLUCINATIONS---\n",
            "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
            "---GRADE GENERATION vs QUESTION---\n",
            "---DECISION: GENERATION ADDRESSES QUESTION---\n",
            "\n",
            " Node 'generate':\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Final Generated Response:\n",
            "\n",
            "Amazon's first quantum computing chip is called Ocelot. It is designed to reduce\n",
            "quantum computing errors by up to 90%, making quantum computers more reliable.\n",
            "This chip is a significant step in the development of practical quantum\n",
            "computing technology.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "inputs = {\"question\": \"What are the main features of Google’s new quantum computer chip?\"}\n",
        "\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Print the node name\n",
        "        print(f\"\\n Node '{key}':\")\n",
        "\n",
        "        # Optional: Print full state at each node (formatted for readability)\n",
        "        if \"keys\" in value:\n",
        "            formatted_text = textwrap.fill(str(value[\"keys\"]), width=80, subsequent_indent=\"    \")\n",
        "            print(f\"   State: {formatted_text}\")\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 80 + \"\\n\")  # Separator for clarity\n",
        "\n",
        "# Final generation output with text wrapping\n",
        "if \"generation\" in value:\n",
        "    print(\"\\n Final Generated Response:\\n\")\n",
        "    print(textwrap.fill(value[\"generation\"], width=80, subsequent_indent=\"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfyZ77vOOBaB",
        "outputId": "2c31525f-a6dc-4497-e3fa-9ee577b9a96a"
      },
      "id": "SfyZ77vOOBaB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---RETRIEVE---\n",
            "\n",
            " Node 'retrieve':\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: GENERATE---\n",
            "\n",
            " Node 'grade_documents':\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "---GENERATE---\n",
            "---CHECK HALLUCINATIONS---\n",
            "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
            "---GRADE GENERATION vs QUESTION---\n",
            "---DECISION: GENERATION ADDRESSES QUESTION---\n",
            "\n",
            " Node 'generate':\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Final Generated Response:\n",
            "\n",
            "Google's new quantum computer chip, Willow, reduces errors and vastly\n",
            "outperforms standard benchmarks. The chip can perform a standard benchmark\n",
            "computation in under five minutes, a task that would take current supercomputers\n",
            "10 septillion years. The more qubits are scaled up in the Willow chip, the lower\n",
            "the rate of error.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run\n",
        "inputs = {\"question\": \"Please compare Google’s New Quantum Computer Chip with Amazon's first quantum computing chip\"}\n",
        "\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Print the node name\n",
        "        print(f\"\\n Node '{key}':\")\n",
        "\n",
        "        # Optional: Print full state at each node (formatted for readability)\n",
        "        if \"keys\" in value:\n",
        "            formatted_text = textwrap.fill(str(value[\"keys\"]), width=80, subsequent_indent=\"    \")\n",
        "            print(f\"   State: {formatted_text}\")\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 80 + \"\\n\")  # Separator for clarity\n",
        "\n",
        "# Final generation output with text wrapping\n",
        "if \"generation\" in value:\n",
        "    print(\"\\n Final Generated Response:\\n\")\n",
        "    print(textwrap.fill(value[\"generation\"], width=80, subsequent_indent=\"\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnLPvjw7OCPt",
        "outputId": "83097dee-bfe5-434b-929a-d15f5496d75b"
      },
      "id": "fnLPvjw7OCPt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---RETRIEVE---\n",
            "\n",
            " Node 'retrieve':\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT NOT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: GENERATE---\n",
            "\n",
            " Node 'grade_documents':\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "---GENERATE---\n",
            "---CHECK HALLUCINATIONS---\n",
            "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
            "---GRADE GENERATION vs QUESTION---\n",
            "---DECISION: GENERATION ADDRESSES QUESTION---\n",
            "\n",
            " Node 'generate':\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Final Generated Response:\n",
            "\n",
            "Google's new quantum computer chip, Willow, reduces errors and vastly\n",
            "outperforms standard benchmarks, cracking a 30-year challenge in the field.\n",
            "Amazon's first quantum computing chip, Ocelot, is claimed to be an important\n",
            "step in the development of useful and reliable quantum computers, lowering the\n",
            "costs of reducing quantum computing errors by up to 90%. Both chips represent\n",
            "significant advancements in quantum computing technology.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}